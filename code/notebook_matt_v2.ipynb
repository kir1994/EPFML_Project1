{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utilities.proj1_helpers import load_csv_data\n",
    "import numpy as np\n",
    "from utilities.pca import compute_pca\n",
    "from utilities.preprocessing import standard_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_csv_data(\"data/train.csv\")\n",
    "y, x, ids = train_data[0], train_data[1], train_data[2]\n",
    "y[np.where(y == -1)] = 0\n",
    "N, D = x.shape\n",
    "print(y.shape, x.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = standard_scaler(x)\n",
    "x_reduced, vectors = compute_pca(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    z = np.exp(t)\n",
    "    return z/(1 + z)\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    z = tx.dot(w)\n",
    "    neg_log_L = np.sum(np.log(1 + np.exp(z))) - y.T.dot(z)\n",
    "    return neg_log_L[0]\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    z = sigmoid(tx.dot(w))[:, 0] - y\n",
    "    grad = tx.T.dot(z)\n",
    "    return grad\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate hessian: TODO\n",
    "    # ***************************************************\n",
    "    z = sigmoid(tx.dot(w))\n",
    "    S_nn= z*(1-z)\n",
    "    H = tx.T.dot(S_nn*tx)\n",
    "    return H\n",
    "\n",
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient, and hessian: TODO\n",
    "    # ***************************************************\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    H = calculate_hessian(y, tx, w)\n",
    "    return loss, grad, H\n",
    "\n",
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient and hessian: TODO\n",
    "    # ***************************************************\n",
    "    loss, grad, H = logistic_regression(y, tx, w)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # update w: TODO\n",
    "    # ***************************************************\n",
    "    # Search for gamma using Armijo's rule\n",
    "    beta = 0.5\n",
    "    sigma = 1e-2\n",
    "    gamma = 1 # step size\n",
    "    d = np.linalg.solve(H, grad) # direction of descent\n",
    "    w_next = w - gamma*d\n",
    "    loss_next = calculate_loss(y, tx, w_next)\n",
    "    while (loss - loss_next) <= - sigma*gamma*grad.T.dot(d):\n",
    "        gamma *= beta\n",
    "        w_next = w - gamma*d\n",
    "        loss_next = calculate_loss(y, tx, w_next)\n",
    "    w = w_next\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter-1):\n",
    "        loss, w = learning_by_newton_method(y, tx, w)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic_regression_newton_method_demo(y, x_reduced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
