{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utilities.proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_csv_data(\"data/train.csv\")\n",
    "y_train, x_train, ids_train = train_data[0], train_data[1], train_data[2]\n",
    "N, D = x_train.shape\n",
    "print(y_train.shape, x_train.shape, ids_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,) (568238, 30) (568238,)\n"
     ]
    }
   ],
   "source": [
    "test_data = load_csv_data(\"data/test.csv\")\n",
    "y_test, x_test, ids_test = test_data[0], test_data[1], test_data[2]\n",
    "print(y_test.shape, x_test.shape, ids_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utilities.implementations import *\n",
    "from utilities.cross_validation import cross_validation, build_k_indices, split_data\n",
    "#from utilities.pca import compute_pca\n",
    "from utilities.preprocessing import standard_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of missing values\n",
    "Too many samples have at least one missing values (around 73%), we have to replace them with the mean (which will be 0 after normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181886 / 250000 = 72.7544 % of samples with missing values\n"
     ]
    }
   ],
   "source": [
    "# Count the number of samples with missing values\n",
    "count = 0\n",
    "for i in range(len(x_train)):\n",
    "    if -999.0 in x_train[i,:]:\n",
    "        count += 1\n",
    "        \n",
    "print(count, \"/\", len(x_train), \"=\", count/len(x_train)*100, \"% of samples with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize and center the data considering ONLY the correct values (not taking in account the values set at -999.0)\n",
    "\n",
    "x_train_mean = np.zeros(D)\n",
    "x_train_std  = np.zeros(D)\n",
    "# Create a boolean mask with False at missing values\n",
    "x_train_mask = (x_train != -999.0)\n",
    "\n",
    "# Loop on the features, compute the mean/std without the missing values\n",
    "for i in range(D):\n",
    "    feature_values = x_train[x_train_mask[:, i], i]\n",
    "    x_train_mean[i] = feature_values.mean()\n",
    "    x_train_std[i]  = feature_values.std()\n",
    "    \n",
    "# Normalize and center the data\n",
    "x = (x_train - x_train_mean) / x_train_std\n",
    "# Set to 0 (the mean) the missing values\n",
    "x[np.invert(x_train_mask)] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "Split the data into train and validation set. We first learn the model on the train set, and then test it on the validation set.\n",
    "\n",
    "The ration gives the percent of the data going to train (ratio = 0.8 means 80% for training and 20% for validating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165000, 30) (85000, 30) \n",
      " 66.0 % of training\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "ratio = 0.66\n",
    "\n",
    "# Choose here if you want the original data (x), or after PCA (x_pc)\n",
    "x_tr, y_tr, x_te, y_te = split_data(x, y_train, ratio, seed)\n",
    "print(x_tr.shape, x_te.shape, \"\\n\", x_tr.shape[0] / (x_tr.shape[0]+x_te.shape[0])*100, \"% of training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and loss\n",
    "Compute here the weight and the resulting loss of the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/499): loss=114369.28479239097, w0=-0.26081000000000004, w1=0.008210225040278137\n",
      "Stochastic Gradient Descent(100/499): loss=82538.99428367298, w0=-0.8819560951008333, w1=-0.01860962796328153\n",
      "Stochastic Gradient Descent(200/499): loss=82390.58330193603, w0=-0.8959900131139303, w1=0.0561188624653027\n",
      "Stochastic Gradient Descent(300/499): loss=82371.59638280456, w0=-0.8997458265699114, w1=0.08019818470151518\n",
      "Stochastic Gradient Descent(400/499): loss=82367.77577816414, w0=-0.9007023034194223, w1=0.08775054966160964\n",
      "Train logistic loss = 82366.7641886\n",
      "Train Accuracy = 0.750139393939\n"
     ]
    }
   ],
   "source": [
    "## Logistic regression\n",
    "\n",
    "# y = {-1; +1} => y = {0; +1}\n",
    "y_tr_log = (y_tr + 1) / 2\n",
    "# Add a \"biais\" to the input\n",
    "tx_tr = np.c_[np.ones(x_tr.shape[0]), x_tr]\n",
    "\n",
    "initial_w = np.zeros(tx_tr.shape[1])\n",
    "max_iters = 500\n",
    "gamma     = 1E-5\n",
    "\n",
    "w, loss_tr = logistic_regression(y_tr_log, tx_tr, initial_w, max_iters, gamma)\n",
    "print(\"Train logistic loss =\", loss_tr)\n",
    "\n",
    "# Compute the accuracy\n",
    "y_tr_pred = predict_labels(w, tx_tr)\n",
    "accuracy_tr = 1. - np.sum(np.absolute(y_tr - y_tr_pred)) / 2. / y_tr.shape[0]\n",
    "print(\"Train Accuracy =\", accuracy_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE loss = 0.360422351629\n",
      "Train Accuracy = 0.721884848485\n"
     ]
    }
   ],
   "source": [
    "## Least-squares\n",
    "\n",
    "# Add a \"biais\" to the input\n",
    "tx_tr = np.c_[np.ones(x_tr.shape[0]), x_tr]\n",
    "\n",
    "w, loss_tr = least_squares(y_tr, tx_tr)\n",
    "print(\"Train MSE loss =\", loss_tr)\n",
    "\n",
    "# Compute the accuracy\n",
    "y_tr_pred = predict_labels(w, tx_tr)\n",
    "accuracy_tr = 1. - np.sum(np.absolute(y_tr - y_tr_pred)) / 2. / y_tr.shape[0]\n",
    "print(\"Train Accuracy =\", accuracy_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Test the learned model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test logistic loss = 42503.7955583\n",
      "Test Accuracy = 0.750505882353\n"
     ]
    }
   ],
   "source": [
    "# Add a \"biais\" to the input\n",
    "tx_te = np.c_[np.ones(x_te.shape[0]), x_te]\n",
    "\n",
    "# y = {-1; +1} => y = {0; +1}\n",
    "y_te_log = (y_te + 1) / 2\n",
    "\n",
    "loss_te = compute_logistic_loss(y_te_log, tx_te, w)\n",
    "print(\"Test logistic loss =\", loss_te)\n",
    "\n",
    "# Compute the accuracy\n",
    "y_te_pred = predict_labels(w, tx_te)\n",
    "accuracy_te = 1. - np.sum(np.absolute(y_te - y_te_pred)) / 2. / y_te.shape[0]\n",
    "print(\"Test Accuracy =\", accuracy_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Kaggle labels\n",
    "Try to predicts the labels of the test set, then create a submission to be posted on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the test set into the same space as the train set\n",
    "In order to use the same weight, we need to send the testing data into the same space as the during the training (i.e. normalization, centering, PCA, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a boolean mask with False at missing values\n",
    "x_test_mask = (x_test != -999.0)\n",
    "\n",
    "# Normalize and center the data\n",
    "x_kaggle = (x_test - x_train_mean) / x_train_std\n",
    "# Set to 0 (the mean) the missing values\n",
    "x_kaggle[np.invert(x_test_mask)] = 0.\n",
    "\n",
    "# Send the data to PCA space\n",
    "x_kaggle_pc = x_kaggle.dot(eigenvectors)\n",
    "\n",
    "# Add a \"bias\" to the input (choose if you want the original, or PCAed data here)\n",
    "tx_kaggle = np.c_[np.ones(x_kaggle.shape[0]), x_kaggle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the labels, and create a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "y_pred = predict_labels(w, tx_kaggle)\n",
    "\n",
    "# Create a sumbission file to be uploaded to the Kaggle competition\n",
    "create_csv_submission(ids_test, y_pred, \"nico_log_subm.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
